---
title: "102B Final Project"
output: html_document
date: "2023-06-15"
---
Libraries
```{r}
library(ggplot2)
library(MASS)
library(tidyverse)
```

Helper Functions
```{r}
bootsampling <- function(x, boot.replicates = B){
  x = as.matrix(x)
  nx = nrow(x)
  bootsamples = replicate(boot.replicates, x[sample.int(nx, replace = TRUE), ])
}

boot.stats <- function(boot.sample, myfun, ...){
  if(is.matrix(boot.sample)){
    theta <- apply(boot.sample, 2, myfun, ...)
  } else {
    theta <- apply(boot.sample, 3, myfun, ...)
  }
  if(is.matrix(theta)){
    return(list(theta = theta, cov = cov(t(theta))))
  } else{
    return(list(theta = theta, se = sd(theta)))
  } }
```


# Problem 1

```{r}
alpha <- 0.05
k <- 1.5
lambda <- 1
#true median
pop_median = lambda* log(2)^(1/k)

#true standard deviation
pop_variance = lambda^2 * (gamma(1 + 2/k) - (gamma(1 + 1/k))^2)
pop_sd = sqrt(pop_variance)
```

## (a)
fixed parameters for case 1~3
NOTE1: instead of using B = 5000, I use B = 500 here to save computation without losing much coverage probability.
NOTE2:Change of B is permitted by Professor George Michailidis according to his email instructions

```{r}
B = 500 
```


### 1. Normal bootstrap confidence interval(CIN)
Design a function to get CIN
```{r}
Get_CIN<- function(n,M,k,lambda,B, alpha  ){
  X = replicate ( M ,  rweibull(n,shape=k,scale=lambda)) # create M = 100 random samples with size n=100

normal.CI = data.frame(matrix(ncol = 2, nrow = 0))
alpha=.05
zval=qnorm(p=alpha/2, lower.tail=FALSE)


for(i in 1:M){
  x <- X[,i]
boot.samples = bootsampling(x,B)
x.median = boot.stats(boot.samples,median)

normal.CI=rbind(normal.CI,cbind(median(x)-zval*x.median$se,median(x)+zval*x.median$se))
} #median(x) from the original sample
# x.median$se correspond to the bootstrapped sample variance
# we are estimate the CI based on our generated sample and its bootstrapped sample

# calculate average length
avg_length = mean(normal.CI[,2]-normal.CI[,1])
#calculate coverage probability
count <-sum(normal.CI[,1]<= pop_median& normal.CI[,2]>= pop_median)
coverage <- count / M

#display results
cat("coverage probility is",coverage, "\n")
cat("average length is:",avg_length)

}


```
#### Case1: n = 100,M =100
```{r}
n <- 100
M <- 100
set.seed(1)
Get_CIN(n,M,k,lambda,B, alpha )

```
#### Case2: n = 100,M =500
```{r}
set.seed(2)
n <- 100
M <-500
Get_CIN(n,M,k,lambda,B, alpha )

```
#### Case3: n = 250,M =100
```{r}
set.seed(3)
n <- 250
M <-100
Get_CIN(n,M,k,lambda,B, alpha )

```
#### Case4: n = 250,M =500
```{r}
set.seed(4)
n <- 250
M <-500
Get_CIN(n,M,k,lambda,B, alpha )

```

###2. Basic bootstrap confidence interval.
Design a function to get CIB
```{r}
Get_CIB<- function(n,M,k,lambda,B, alpha  ){
   X = replicate ( M ,  rweibull(n,shape=k,scale=lambda)) # create M = 100 random samples with size n=100

# create  a data frame for CI
Basic.CI = data.frame(matrix(ncol = 2, nrow = 0))
for (i in 1:M){
  x <-X[,i]
  boot.samples = bootsampling(x,B) # for each random sample, bootsampling with B = 5000 replicates
  x.median = boot.stats(boot.samples,median) # bootstrapped mean for Xm
  #calculate Confidence Interval for each Xm
Basic.CI=rbind(Basic.CI,cbind(2 * median(x)
 - quantile(x.median$theta, probs = 1 - alpha / 2, names = FALSE), 2 * median(x) - quantile(x.median$theta, probs = alpha / 2, names = FALSE)))
}

# calculate average length
avg_length = mean(Basic.CI[,2]-Basic.CI[,1])
#calculate coverage probability
count <-sum(Basic.CI[,1]<= pop_median& Basic.CI[,2]>= pop_median)
coverage <- count / M

#display results
cat("coverage probility is",coverage, "\n")
cat("average length is:",avg_length)

}

```
#### Case1: n = 100,M =100
```{r}
n <- 100
M <- 100
set.seed(5)
Get_CIB(n,M,k,lambda,B, alpha )

```

#### Case2: n = 100,M =500
```{r}
set.seed(6)
n <- 100
M <-500
Get_CIB(n,M,k,lambda,500, alpha )

```
#### Case3: n = 250,M =100
```{r}
set.seed(7)
n <- 250
M <-100
Get_CIB(n,M,k,lambda,B, alpha )

```
#### Case4: n = 250,M =500
```{r}
set.seed(8)
n <- 250
M <-500
Get_CIB(n,M,k,lambda,B, alpha )


```

### 3. Percentile bootstrap confidence interval.
Design a function to get CIP
```{r}
Get_CIP<- function(n,M,k,lambda,B, alpha  ){
   X = replicate ( M ,  rweibull(n,shape=k,scale=lambda)) # create M = 100 random samples with size n=100

# create  a data frame for CI
Percentile.CI = data.frame(matrix(ncol = 2, nrow = 0))

for (i in 1:M){
  x<-X[,i]
  boot.samples = bootsampling(x,B) # for each random sample, bootsampling with B = 5000 replicates
  x.median = boot.stats(boot.samples,median) # bootstrapped mean
  #calculate Confidence Interval for each Xm
Percentile.CI=rbind(Percentile.CI,cbind(quantile(x.median$theta, probs = alpha / 2, names = FALSE),
             quantile(x.median$theta, probs = 1 - alpha / 2, names = FALSE)))

}

# calculate average length
avg_length = mean(Percentile.CI[,2]-Percentile.CI[,1])
#calculate coverage probability
count <-sum(Percentile.CI[,1]<= pop_median& Percentile.CI[,2]>= pop_median)
coverage <- count / M

#display results
cat("coverage probility is",coverage, "\n")
cat("average length is:",avg_length)
}
```
#### Case1: n = 100,M =100
```{r}
n <- 100
M <- 100
set.seed(9)
Get_CIP(n,M,k,lambda,B, alpha )

```

#### Case2: n = 100,M =500
```{r}
set.seed(10)
n <- 100
M <-500
Get_CIP(n,M,k,lambda,B, alpha )

```
#### Case3: n = 250,M =100
```{r}
set.seed(11)
n <- 250
M <-100
Get_CIP(n,M,k,lambda,B, alpha )

```
#### Case4: n = 250,M =500
```{r}
set.seed(12)
n <- 250
M <-500
Get_CIP(n,M,k,lambda,B, alpha )

```
### 4. Bootstrap-t (studentized) confidence interval.

Design a function to get CIS
```{r}
Get_CIS<- function(n,M,k,lambda,B, alpha  ){
   X = replicate ( M ,  rweibull(n,shape=k,scale=lambda)) # create M = 100 random samples with size n=100

# create  a data frame for CI
studentized.CI = data.frame(matrix(ncol = 2, nrow = 0))

for(i in 1:M){
  x <- X[,i]
boot.samples = bootsampling(x,B)

x.median = boot.stats(boot.samples,median)

# obtain the standard deviation of the sample statistic 
# for the l-th bootstrap sample

boot.samples.SE = rep(0,ncol(boot.samples))

for (l in 1:ncol(boot.samples) ) {
  # obtain bootstrap samples from original bootstrap sample l
  iter.boot.samples = bootsampling(boot.samples[,l],B)
  # get the std dev of the bootstrap sampling distribution based on the l-th bootstrap sampe
  boot.samples.SE[l] = boot.stats(iter.boot.samples,median)$se
}

# obtain the T-statistic
T = (x.median$theta-median(x)) / boot.samples.SE

# obtain the quantiles of the distribution of the T-statistic
qval = quantile(T,probs=c(alpha/2,1-alpha/2))
  
# construct the studentized.CI

studentized.CI=rbind(studentized.CI,
                cbind(median(x)-qval[2]*x.median$se,median(x)-qval[1]*x.median$se))
}


# calculate average length
avg_length = mean(studentized.CI[,2]-studentized.CI[,1])
#calculate coverage probability
count <-sum(studentized.CI[,1]<= pop_median& studentized.CI[,2]>= pop_median)
coverage <- count / M

#display results
cat("coverage probility is",coverage, "\n")
cat("average length is:",avg_length)
}
```
NOTE: given there are two iteration within GIS, its run time is significantly higer than the others but achieve very high coverage probability with few replicates B. After manual tuning, I use B = 50 to do the studentdized test alone.  
```{r}
B = 50
```

#### Case1: n = 100,M =100
```{r}
n <- 100
M <- 100
set.seed(13)
Get_CIS(n,M,k,lambda,B, alpha )

```

#### Case2: n = 100,M =500
```{r}
set.seed(14)
n <- 100
M <-500
Get_CIS(n,M,k,lambda,B, alpha )

```
#### Case3: n = 250,M =100
```{r}
set.seed(15)
n <- 250
M <-100
Get_CIS(n,M,k,lambda,B, alpha )

```
#### Case4: n = 250,M =500
```{r}
set.seed(16)
n <- 250
M <-500
Get_CIS(n,M,k,lambda,B, alpha )

```
Description of the part(a)


3. Report the count/coverage and the average length for each of the four types
of CIs constructed"

1. I calculate the population median of our simulated Weibull distribution first given k = 1.5 and lambda = 1.
2. For each type of Confidence Interval (CI), coverage probability and the average length of CI:
  + I first generate M = 100 random samples based on Weibull distribution  
  + Then I wrote a for loop to calculate CI for all random sample X[,i]. On each random sample, I bootstrap them for B = 500 replicated, and I calculate the bootstrapped sample median and the standard deviation of it for each bootstrapped sample on each X[,i]
    + For CIN, I followed CI(mean(X[,i]) - z_value\*(boot.sample median X[,i])\$SE,  median(X[,i])+ z_value*(boot.sample median of X[,i])\$SE)
    + For CIB, I calculated CI with 2\*(sample median of X[,i])-the (1-alpha / 2) quantile of bootstrapped sample median on X[,i]; and  2\*(sample.statistics of X[,i])+the (alpha / 2) quantile of bootstrapped sample median on X[,i]
    + For CIP, the calculation of CI is simple: use 1-(1-alpha / 2) quantile of bootstrapped sample median on X[,i] to calculate the lower bound, and (alpha / 2) quantile of bootstrapped sample median on X[,i] to calculate the upper bound. CI = (lower bound, upper bound).
    + For CIS:
      +


1. I calculate the population median of our simulated Weibull distribution first given k = 1.5 and lambda = 1.
2. For each type of Confidence Interval (CI), coverage probability and the average length of CI:
  + I first generate M = 100 random samples based on Weibull distribution  
  + Then I wrote a for loop to calculate CI for all random sample X[,i]. On each random sample, I bootstrap them for B = 500 replicated, and I calculate the bootstrapped sample median and the standard deviation of it for each bootstrapped sample on each X[,i]
    + For CIN, I followed CI(mean(X[,i]) - z_value\*(boot.sample median X[,i])\$SE,  median(X[,i])+ z_value*(boot.sample median of X[,i])\$SE)
    + For CIB, I calculated CI with 2\*(sample median of X[,i])-the (1-alpha / 2) quantile of bootstrapped sample median on X[,i]; and  2\*(sample.statistics of X[,i])+the (alpha / 2) quantile of bootstrapped sample median on X[,i]
    + For CIP, the calculation of CI is simple: use 1-(1-alpha / 2) quantile of bootstrapped sample median on X[,i] to calculate the lower bound, and (alpha / 2) quantile of bootstrapped sample median on X[,i] to calculate the upper bound. CI = (lower bound, upper bound).
    + For CIS:
      + After we get the boot.sample on X[,i] and the median of boot.sample (x.median). We bootstrap on boot.sample to calculate the standard deviation of the bootstrapped sample on a boot.sample (iter.boot.samples) and its SE (boot.samples.SE).
      + Using these to calculate T-statistics, T = ( the bootstrapped medians based on X[,i] - median(X[,i]))/ boot.samples.SE
      + calculate the q_value (qval) by the quantile of T distribution on (alpha/2 ) and (1-alpha/2)
      + Generate studentized CI= (median(x)- qval[2]\*x.median\$se,median(x)+ qval[2]*x.median\$se
    + For each CI
      + We calculate its coverage probability by counting how many of our simulated CI contains the population mean, and divided it by M the number of random samples
      + Calculate the average CI length by using the upper bound (CI[,2]) minus lower bound (CI[,1])
3. Report Average Length of CI and Coverage Probability of Each CI

|Case        | Info                 |     CIN   | CIB        | CIP       | CIS        |
|:-----------|:---------------------|:---------:|:----------:|:---------:|:----------:|
|n=100,M=100 |Coverage Probability  | 0.92      |0.89        |0.93       |0.91        |
|n=100,M=100 |Average Length        | 0.2975732 |0.3004658   |0.286607   |0.3065558   |
|n=100,M=500 |Coverage Probability  |0.95       |0.868       |0.944      |0.898       |
|n=100,M=500 |Average Length        |0.3028203  |0.2920194   |0.2914456  |0.3139956   |
|n=250,M=100 |Coverage Probability  |0.96       |0.87        |0.96       |0.84        |
|n=250,M=100 |Average Length        |0.184724   |0.1786801   |0.1846978  |0.1934652   |
|n=250,M=500 |Coverage Probability  |0.944      |0.892       |0.962      |0.882       |
|n=250,M=500 |Average Length        |0.1874409  |0.1862028   |0.1859115  |0.186187    | 

## (b)
fixed parameters for case CIN, CIB, CIP
NOTE: instead of using B = 5000, I use B = 500 here to save computation without losing much coverage probability
```{r}
alpha <- 0.05
k <- 1.5
lambda <- 1
B = 500 
```

### 1. Normal bootstrap confidence interval(CIN)
Design a function to get CIN
```{r}

Get_CIN<- function(n,M,k,lambda,B, alpha  ){
  X = replicate ( M ,  rweibull(n,shape=k,scale=lambda)) # create M = 100 random samples with size n=100

normal.CI = data.frame(matrix(ncol = 2, nrow = 0))
alpha=.05
zval=qnorm(p=alpha/2, lower.tail=FALSE)


for(i in 1:M){
  x <- X[,i]
boot.samples = bootsampling(x,B)
x.sd = boot.stats(boot.samples,sd)

normal.CI=rbind(normal.CI,cbind(sd(x)-zval*x.sd$se,sd(x)+zval*x.sd$se))
} #median(x) from the original sample
# x.median$se correspond to the bootstrapped sample variance
# we are estimate the CI based on our generated sample and its bootstrapped sample

# calculate average length
avg_length = mean(normal.CI[,2]-normal.CI[,1])
#calculate coverage probability
count <-sum(normal.CI[,1]<= pop_sd& normal.CI[,2]>= pop_sd)
coverage <- count / M

#display results
cat("coverage probility is",coverage, "\n")
cat("average length is:",avg_length)

}


```
#### Case1: n = 100,M =100
```{r}
n <- 100
M <- 100
set.seed(17)
Get_CIN(n,M,k,lambda,B, alpha )

```
#### Case2: n = 100,M =500
```{r}
set.seed(18)
n <- 100
M <-500
Get_CIN(n,M,k,lambda,B, alpha )

```
#### Case3: n = 250,M =100
```{r}
set.seed(19)
n <- 250
M <-100
Get_CIN(n,M,k,lambda,B, alpha )

```
#### Case4: n = 250,M =500
```{r}
set.seed(20)
n <- 250
M <-500
Get_CIN(n,M,k,lambda,B, alpha )

```

###2. Basic bootstrap confidence interval.
Design a function to get CIB
```{r}
Get_CIB<- function(n,M,k,lambda,B, alpha  ){
   X = replicate ( M ,  rweibull(n,shape=k,scale=lambda)) # create M = 100 random samples with size n=100

# create  a data frame for CI
Basic.CI = data.frame(matrix(ncol = 2, nrow = 0))
for (i in 1:M){
  x <-X[,i]
  boot.samples = bootsampling(x,B) # for each random sample, bootsampling with B = 5000 replicates
  x.sd = boot.stats(boot.samples,sd) # bootstrapped mean for Xm
  #calculate Confidence Interval for each Xm
Basic.CI=rbind(Basic.CI,cbind(2 * sd(x)
 - quantile(x.sd$theta, probs = 1 - alpha / 2, names = FALSE), 2 * sd(x) - quantile(x.sd$theta, probs = alpha / 2, names = FALSE)))
}

# calculate average length
avg_length = mean(Basic.CI[,2]-Basic.CI[,1])
#calculate coverage probability
count <-sum(Basic.CI[,1]<= pop_sd& Basic.CI[,2]>= pop_sd)
coverage <- count / M

#display results
cat("coverage probility is",coverage, "\n")
cat("average length is:",avg_length)

}

```
#### Case1: n = 100,M =100
```{r}
n <- 100
M <- 100
set.seed(21)
Get_CIB(n,M,k,lambda,B, alpha )

```

#### Case2: n = 100,M =500
```{r}
set.seed(22)
n <- 100
M <-500
Get_CIB(n,M,k,lambda,B, alpha )

```
#### Case3: n = 250,M =100
```{r}
set.seed(23)
n <- 250
M <-100
Get_CIB(n,M,k,lambda,B, alpha )

```
#### Case4: n = 250,M =500
```{r}
set.seed(24)
n <- 250
M <-500
Get_CIB(n,M,k,lambda,B, alpha )


```

### 3. Percentile bootstrap confidence interval.
Design a function to get CIP
```{r}
Get_CIP<- function(n,M,k,lambda,B, alpha  ){
   X = replicate ( M ,  rweibull(n,shape=k,scale=lambda)) # create M = 100 random samples with size n=100

# create  a data frame for CI
Percentile.CI = data.frame(matrix(ncol = 2, nrow = 0))

for (i in 1:M){
  x<-X[,i]
  boot.samples = bootsampling(x,B) # for each random sample, bootsampling with B = 5000 replicates
  x.sd = boot.stats(boot.samples,sd) # bootstrapped mean
  #calculate Confidence Interval for each Xm
Percentile.CI=rbind(Percentile.CI,cbind(quantile(x.sd$theta, probs = alpha / 2, names = FALSE),
             quantile(x.sd$theta, probs = 1 - alpha / 2, names = FALSE)))

}

# calculate average length
avg_length = mean(Percentile.CI[,2]-Percentile.CI[,1])
#calculate coverage probability
count <-sum(Percentile.CI[,1]<= pop_sd& Percentile.CI[,2]>= pop_sd)
coverage <- count / M

#display results
cat("coverage probility is",coverage, "\n")
cat("average length is:",avg_length)
}
```
#### Case1: n = 100,M =100
```{r}
n <- 100
M <- 100
set.seed(25)
Get_CIP(n,M,k,lambda,B, alpha )

```

#### Case2: n = 100,M =500
```{r}
set.seed(26)
n <- 100
M <-500
Get_CIP(n,M,k,lambda,B, alpha )

```
#### Case3: n = 250,M =100
```{r}
set.seed(27)
n <- 250
M <-100
Get_CIP(n,M,k,lambda,B, alpha )

```
#### Case4: n = 250,M =500
```{r}
set.seed(28)
n <- 250
M <-500
Get_CIP(n,M,k,lambda,B, alpha )

```
### 4. Bootstrap-t (studentized) confidence interval.

Design a function to get CIS
```{r}
Get_CIS<- function(n,M,k,lambda,B, alpha  ){
   X = replicate ( M ,  rweibull(n,shape=k,scale=lambda)) # create M = 100 random samples with size n=100

# create  a data frame for CI
studentized.CI = data.frame(matrix(ncol = 2, nrow = 0))

for(i in 1:M){
  x <- X[,i]
boot.samples = bootsampling(x,B)

x.sd = boot.stats(boot.samples,sd)

# obtain the standard deviation of the sample statistic 
# for the l-th bootstrap sample

boot.samples.SE = rep(0,ncol(boot.samples))

for (l in 1:ncol(boot.samples) ) {
  # obtain bootstrap samples from original bootstrap sample l
  iter.boot.samples = bootsampling(boot.samples[,l],B)
  # get the std dev of the bootstrap sampling distribution based on the l-th bootstrap sampe
  boot.samples.SE[l] = boot.stats(iter.boot.samples,sd)$se
}

# obtain the T-statistic
T = (x.sd$theta-sd(x)) / boot.samples.SE

# obtain the quantiles of the distribution of the T-statistic
qval = quantile(T,probs=c(alpha/2,1-alpha/2))
  
# construct the studentized.CI

studentized.CI=rbind(studentized.CI,
                cbind(sd(x)-qval[2]*x.sd$se,sd(x)-qval[1]*x.sd$se))
}


# calculate average length
avg_length = mean(studentized.CI[,2]-studentized.CI[,1])
#calculate coverage probability
count <-sum(studentized.CI[,1]<= pop_sd& studentized.CI[,2]>= pop_sd)
coverage <- count / M

#display results
cat("coverage probility is",coverage, "\n")
cat("average length is:",avg_length)
}
```

NOTE: given there are two iteration within GIS, its run time is significantly higer than the others but achieve very high coverage probability with few replicates B. After manual tuning, I use B = 50 to do the studentdized test alone.  

```{r}
B = 50
```
#### Case1: n = 100,M =100
```{r}
n <- 100
M <- 100
set.seed(29)
Get_CIS(n,M,k,lambda,B, alpha )

```

#### Case2: n = 100,M =500
```{r}
set.seed(30)
n <- 100
M <-500
Get_CIS(n,M,k,lambda,B, alpha )

```
#### Case3: n = 250,M =100
```{r}
set.seed(31)
n <- 250
M <-100
Get_CIS(n,M,k,lambda,B, alpha )

```
#### Case4: n = 250,M =500
```{r}
set.seed(32)
n <- 250
M <-500
Get_CIS(n,M,k,lambda,B, alpha )

```


1. I calculate the population standard deviation(sd) of our simulated Weibull distribution first given k = 1.5 and lambda = 1.
2. For each type of Confidence Interval (CI), coverage probability and the average length of CI:
  + I first generate M = 100 random samples based on Weibull distribution  
  + Then I wrote a for loop to calculate CI for all random sample X[,i]. On each random sample, I bootstrap them for B = 500 replicated, and I calculate the bootstrapped sample sd and the standard deviation of the sd for each bootstrapped sample on each X[,i]
    + For CIN, I followed CI(mean(X[,i]) - z_value\*(boot.sample sd X[,i])\$SE,  sd(X[,i])+ z_value*(boot.sample sd of X[,i])\$SE)
    + For CIB, I calculated CI with 2\*(sample sd of X[,i])-the (1-alpha / 2) quantile of bootstrapped sample sd on X[,i]; and  2\*(sample.statistics of X[,i])+the (alpha / 2) quantile of bootstrapped sample sd on X[,i]
    + For CIP, the calculation of CI is simple: use 1-(1-alpha / 2) quantile of bootstrapped sample sd on X[,i] to calculate the lower bound, and (alpha / 2) quantile of bootstrapped sample sd on X[,i] to calculate the upper bound. CI = (lower bound, upper bound).
    + For CIS:
      + After we get the boot.sample on X[,i] and the sd of boot.sample (sd). We bootstrap on boot.sample to calculate the standard deviation of the bootstrapped sample on a boot.sample (iter.boot.samples) and its SE (boot.samples.SE).
      + Using these to calculate T-statistics, T = ( the bootstrapped medians based on X[,i] - sd(X[,i]))/ boot.samples.SE
      + calculate the q_value (qval) by the quantile of T distribution on (alpha/2 ) and (1-alpha/2)
      + Generate studentized CI= (sd(x)- qval[2]\*x.sd\$se,sd(x)+ qval[2]*x.sd\$se
    + For each CI
      + We calculate its coverage probability by counting how many of our simulated CI contains the population mean, and divided it by M the number of random samples
      + Calculate the average CI length by using the upper bound (CI[,2]) minus lower bound (CI[,1])
3. Report Average Length of CI and Coverage Probability of Each CI

|Case        | Info                 |     CIN   | CIB        | CIP       | CIS        |
|:-----------|:---------------------|:---------:|:----------:|:---------:|:----------:|
|n=100,M=100 |Coverage Probability  | 0.94      |0.94        |0.85       |0.86        |
|n=100,M=100 |Average Length        | 0.2065513 |0.1993732   |0.2006768  |0.2292618   |
|n=100,M=500 |Coverage Probability  |0.914      |0.92        |0.892      |0.89        |
|n=100,M=500 |Average Length        |0.2052708  |0.1998169   |0.2005602  |0.2273378   |
|n=250,M=100 |Coverage Probability  |0.92       |0.92        |0.92       |0.9         |
|n=250,M=100 |Average Length        |0.1359116  |0.1372693   |0.1332743  |0.1518025   |
|n=250,M=500 |Coverage Probability  |0.936      |0.91        |0.918      |0.91        |
|n=250,M=500 |Average Length        |0.135217   |0.1327013   |0.1321043  |0.1378189   |






# Problem 2

## (a)
```{r}

n <- 100000  # Number of observations
p <- 20      # Number of predictors

# 1. True regression coefficient = 3
beta_true <- rep(3, p)

# 2.Correlation matrix for predictors
R <- matrix(c(rep(0.99, p/2), rep(0.9, p/2)), p, p)
diag(R) <- 1

# 2. Generate predictors with mean 0 and correlation matrix R
X <- mvrnorm(n, mu= rep(0, p), R)

# 3. Error term
epsilon <- rnorm(n, mean = 0, sd = 1.25)


# Generate response variable
y <- X %*% beta_true + epsilon #including information of the beta_true and epsilon



```

## (b)
gradient descent (GD) function:
```{r}
gradientDesc = function(y, X, startpoint, stepsize, 
                        conv_threshold,epsilon, tau, max_iter) {
  
  old.point = startpoint
  gradient = (t(X)%*%X%*%old.point - t(X)%*%y)
  
  # determine stepsize by backtracking line search
    while (t(y - X%*%(old.point-stepsize*gradient))%*%(y-X%*%(old.point-stepsize*gradient)) >
                t(y - X%*%old.point)%*%(y-X%*%old.point) - epsilon * stepsize * t(gradient) %*% gradient )            
     {
    stepsize = tau * stepsize
  }
  
  new.point = old.point - stepsize * gradient

  old.value.function = t(y - X%*%old.point)%*%(y-X%*%old.point)
  
  converged = F
  iterations = 0
  
  while(converged == F) {
    ## Implement the gradient descent algorithm
    old.point = new.point
    
    gradient = t(X) %*% X %*% old.point - t(X)%*%y
    
    # determine stepsize by backtracking line search
    
    while (t(y - X%*%(old.point-stepsize*gradient))%*%(y-X%*%(old.point-stepsize*gradient)) >
           t(y - X%*%old.point)%*%(y-X%*%old.point) - epsilon * stepsize * t(gradient) %*% gradient )            
    {
      stepsize = tau * stepsize
    }
    
    new.point = old.point - stepsize * gradient
    
    new.value.function = t(y - X%*%new.point)%*%(y-X%*%new.point)
    
    if( abs(old.value.function - new.value.function) <=conv_threshold){
      converged = T
    }
    
    data.output = data.frame(iteration = iterations,
                       old.value.function = old.value.function,
                       new.value.function = new.value.function,
                       old.point=old.point,
                       new.point=new.point,
                       stepsize = stepsize
                       )
    iters <- NULL # because we only need the new.point (beta_hat) of the last/stop iteration as our estimation. This can save memory.
    if(exists("iters")) {
      iters <- rbind(iters, data.output)
    } else {
      iters = data.output
    }
    
    iterations = iterations + 1
    old.value.function = new.value.function
    
    if(iterations >= max_iter) break
  }
  return(list(converged = converged, 
              num_iterations = iterations, 
              old.value.function = old.value.function,
              new.value.function = new.value.function,
              coefs = new.point,
              stepsize = stepsize,
              iters = iters,
              converge = converged))
}
```

Running GD
```{r}
# starting point
startpoint=c(rep(0,p)) # startpoint
#  define the step size
mystepsize=0.001
# here we define the tolerance of the convergence criterion
mytol = 0.00000001 
epsilon = 0.9
tau = 0.5

set.seed(33)
results_GD = gradientDesc(y, X, startpoint, 
                       mystepsize, mytol,epsilon, tau, max_iter= 1000000)

```
Least Square Model (LS)
```{r}
LS_model <- lm(y~X)
#Compare Two Models
df_GD.LS = data.frame(cbind(results_GD$coefs,LS_model$coefficients))
names(df_GD.LS) <- c("GD", "LS")
df_GD.LS
cat("\nconverged?",results_GD$converge, "\n")

```


Discussion of GD:

* step size selection: I use backtracking line search (BLS) method to search the step size. I start we a relatively large step size and use parameter tau to tune it within BLS untill it gets optimal. 

* Number of Iterations: 
```{r}
cat("Number of iteration of GD:",results_GD$num_iterations)
```

* Comment on Accuracy: Under the given conditions, __ model performs better than __ model because its estimates are more close to Beta_True. We observe that __ model's estiamtes spread closely around beta_true = 3, while __ model has some estimates far away from 3, like __ . Therefore, we conclude __ model is more accurate in the given condition. 

## (c)
gradient descent with Polyak’s heavy ball adjustment (GDP) :
```{r}
gradientDescBLSPolyak = function(y, X, startpoint, stepsize_0, 
                  tau, epsilon,momentum, conv_threshold,max_iter) {
  
  old.point = startpoint
  older.point = old.point 
  
  stepsize = stepsize_0
  gradient = (t(X)%*%X%*%old.point - t(X)%*%y)
  
  # determine stepsize by backtracking line search at iter = 0
  
  while (t(y - X%*%(old.point-stepsize*gradient))%*%(y-X%*%(old.point-stepsize*gradient)) >
                t(y - X%*%old.point)%*%(y-X%*%old.point) - epsilon * stepsize * t(gradient) %*% gradient )            
     {
    stepsize = tau * stepsize
  }
  
  new.point = old.point - stepsize * gradient + momentum * (old.point - older.point)


  old.value.function = t(y - X%*%new.point)%*%(y-X%*%new.point)
  
  converged = F
  iterations = 0
  
  while(converged == F) {
    ## Implement the gradient descent algorithm
    older.point = old.point
    old.point = new.point
  
    gradient = t(X)%*%X%*%old.point - t(X)%*%y
    
    # here we check how to pick the stepsize at iteration k
    stepsize = stepsize_0
    
    iters <- NULL
    
    while (t(y - X%*%(old.point-stepsize*gradient))%*%(y-X%*%(old.point-stepsize*gradient)) >
           t(y - X%*%old.point)%*%(y-X%*%old.point) - epsilon * stepsize * t(gradient) %*% gradient )            
    {
      stepsize = tau * stepsize
    }
    
       new.point = old.point - stepsize*gradient + momentum*(old.point - older.point)

    
    new.value.function = t(y - X%*%new.point)%*%(y-X%*%new.point)
    
    
    if( abs(old.value.function - new.value.function) <= conv_threshold) {
      converged = T
    }
    
    data.output = data.frame(iteration = iterations,
                       old.value.function = old.value.function,
                       new.value.function = new.value.function,
                       old.point=old.point, new.point=new.point,
                       stepsize = stepsize
                       )
   
    if(exists("iters")) {
      iters <- rbind(iters, data.output)
    } else {
      iters = data.output
    }
    
    iterations = iterations + 1
    old.value.function = new.value.function
    
    if(iterations >= max_iter){break}
  }
  return(list(converged = converged, 
              num_iterations = iterations, 
              old.value.function = old.value.function,
              new.value.function = new.value.function,
              coefs = new.point,
              stepsize = stepsize,
              iters = iters,
              converge = converged))
}

```


```{r}
# given y, X
startpoint=c(rep(0,p)) # startpoint
stepsize_0 = 0.001 #because we will use BLS to get optimal step size, step initial step size larger
tau = 0.5# tau for backtracking line search
epsilon = 0.9
momentum = 0.01
tol = 0.0000000001 #  convergence threshold

# epsilon for backtracking line search


set.seed(34)
 results_GDP <- gradientDescBLSPolyak(y, X, startpoint, stepsize_0, 
                  tau, epsilon,momentum, tol,max_iter = 1000000)
```
To compare the part(c) results with part (b)
```{r}
df_GD.GDP = data.frame(cbind(results_GD$coefs, results_GDP$coefs))
names(df_GD.GDP) <- c("GD", "GDP")
df_GD.GDP

cat("\nconverged?",results_GDP$converge, "\n")
```

Discussion of GD:

* step size selection: I use backtracking line search (BLS) method to search the step size. I start we a relatively large step size and use parameter tau to tune it within BLS untill it gets optimal. 

* Number of Iterations: 
```{r}
cat("Number of iteration of GDP:",results_GDP$num_iterations)
```

* Comment on Accuracy: Under the given conditions, __ model performs better than __ model because its estimates are more close to Beta_True. We observe that __ model's estiamtes spread closely around beta_true = 3, while __ model has some estimates far away from 3, like __ . Therefore, we conclude __ model is more accurate in the given condition. 

## (d)
Stochastic Gradient Descent with Polyak’s heavy ball adjustment (SGDP)


```{r}
# mb = minimum bach = Q

SGDP <- function(y, X, startpoint, stepsize,  tau,epsilon,momentum,mb, conv_threshold, max_iter) {

  mini_batch = ceiling(length(y) * mb) #cieling bc batchsize needs to be an integer
  
  z = cbind(y, X)
  # shuffle the data and select a mini batch
  shuffle.sample = z[sample(mini_batch, replace = FALSE), ]  
  
  ys = shuffle.sample[, 1]
  Xs = shuffle.sample[, 2:ncol(shuffle.sample)]
  
  
  
  old.point = startpoint
  older.point = old.point 
  
  stepsize = stepsize
  gradient = (t(Xs) %*% Xs %*% old.point - t(Xs) %*% ys) #gradient [(X'X )βk − X'y ]
  
   while (t(ys - Xs %*% (old.point - stepsize * gradient)) %*% (ys - Xs %*% (old.point - stepsize * gradient)) >
           t(ys - Xs %*% old.point) %*% (ys - Xs %*% old.point) - epsilon * stepsize * t(gradient) %*% gradient)            
    {
      stepsize = tau * stepsize
    }

  new.point = old.point - stepsize * gradient + momentum * (old.point - older.point) #Hence, for the regression problem the update becomes βk+1 = βk − η*[(X'X )βk − X'y ] + ξ(βk − βk−1)

  old.value.function = t(ys - Xs %*% old.point) %*% (ys-Xs %*% old.point)  # SSE(βk) = (Ys - Xs'βk+1)'(Ys - Xs'βk+1) # update old point
  
  converged = F
  iterations = 0
  
  while(converged == F) {
  
    # shuffle the data and select a mini batch
shuffle.sample = z[sample(mini_batch, replace = FALSE), ]  
  
  ys = shuffle.sample[, 1]
  Xs = shuffle.sample[, 2:ncol(shuffle.sample)]
  
    ## Implement the stochastic gradient descent algorithm
    older.point = old.point #is this update correct?
    old.point = new.point
  
    gradient = t(Xs) %*% Xs %*% old.point - t(Xs) %*% ys
    
    # determine stepsize by backtracking line search
    # SSE(βk ) − SSE(βk−1)) > tol 
    while (t(ys - Xs %*% (old.point - stepsize * gradient)) %*% (ys - Xs %*% (old.point - stepsize * gradient)) >
           t(ys - Xs %*% old.point) %*% (ys - Xs %*% old.point) - epsilon * stepsize * t(gradient) %*% gradient)            
    {
      stepsize = tau * stepsize
    }
    
    new.point = old.point - stepsize*gradient + momentum*(old.point - older.point)
    
    new.value.function = t(ys - Xs %*% new.point) %*% (ys - Xs %*% new.point) #both update new.point
    
    
    if( abs(old.value.function - new.value.function) <= conv_threshold) {
      converged = T
    }
    
    data.output = data.frame(iteration = iterations,
                       old.value.function = old.value.function,
                       new.value.function = new.value.function,
                       old.point=old.point, new.point=new.point,
                       stepsize = stepsize
                       )
    
    if(exists("iters")) {
      iters <- rbind(iters, data.output)
    } else {
      iters = data.output
    }
    
    iterations = iterations + 1
    # update?
    old.value.function = new.value.function
    
    if(iterations >= max_iter) break
  }
  return(list(converged = converged, 
              num_iterations = iterations, 
              old.value.function = old.value.function,
              new.value.function = new.value.function,
              coefs = new.point,
              stepsize = stepsize,
              iters = iters,
              converge = converged))
}

```



```{r}
#Given y, X
startpoint=c(rep(0,p)) # startpoint
mystepsize=5 # define the stepsize_0
tau = 0.5 # tau for backtracking line search
epsilon = 0.9# epsilon for backtracking line search
momentum =0.1 # momentum
mb = 0.002# minibatch size
mytol = 1e-8 # define conv_threshold

set.seed(35)
results_SGDP <- SGDP(y, X, startpoint, mystepsize,  tau,epsilon,momentum,mb, mytol, max_iter = 100000) 
cat("\nconverged?",results_SGDP$converge, "\n")

```
To compare the part(d) results with part (b) and (c)
```{r}
df_SGDP.GD.GDP = data.frame(cbind(results_SGDP$coefs
,results_GD$coefs, results_GDP$coefs))
names(df_SGDP.GD.GDP) <- c("SGDP","GD", "GDP")
df_SGDP.GD.GDP
```

Discussion of SGDP:

* step size selection: I use backtracking line search (BLS) method to search the step size. I start we a relatively large step size and use parameter tau to tune it within BLS untill it gets optimal. 

* Number of Iterations: 
```{r}
cat("Number of iteration of SGDP:",results_SGDP$num_iterations)
```

* Comment on Accuracy: Under the given conditions, __ model performs better than __ model because its estimates are more close to Beta_True. We observe that __ model's estiamtes spread closely around beta_true = 3, while __ model has some estimates far away from 3, like __ . Therefore, we conclude __ model is more accurate in the given condition. 

